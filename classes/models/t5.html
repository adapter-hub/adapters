<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>T5 &mdash; AdapterHub  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />

  
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Vision Transformer (ViT)" href="vit.html" />
    <link rel="prev" title="RoBERTa" href="roberta.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            AdapterHub
              <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../transitioning.html">Transitioning from <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Adapter Methods</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview and Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html">Adapter Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../method_combinations.html">Method Combinations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multi_task_methods.html">Multi Task Methods</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../adapter_composition.html">Adapter Activation and Composition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../merging_adapters.html">Merging Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prediction_heads.html">Prediction Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../embeddings.html">Embeddings</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Loading and Sharing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../huggingface_hub.html">Integration with Hugging Face’s Model Hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../model_overview.html">Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugin_interface.html">Custom Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="beit.html">BEiT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert-generation.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="clip.html">CLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="deberta_v2.html">DeBERTa-v2</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="gptj.html">EleutherAI GPT-J-6B</a></li>
<li class="toctree-l1"><a class="reference internal" href="llama.html">LLaMA</a></li>
<li class="toctree-l1"><a class="reference internal" href="mistral.html">Mistral</a></li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="plbart.html">PLBART</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">T5</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#t5adaptermodel">T5AdapterModel</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="vit.html">Vision Transformer (ViT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="whisper.html">Whisper</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="xmod.html">X-MOD</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_layer.html">Adapter Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_model_interface.html">Adapter Model Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_utils.html">Adapter Utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to AdapterHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/adding_adapter_methods.html">Adding Adapter Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/adding_adapters_to_a_model.html">Adding Adapters to a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../extending.html">Extending the Library</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">AdapterHub</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">T5</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/classes/models/t5.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="t5">
<h1>T5<a class="headerlink" href="#t5" title="Permalink to this heading"></a></h1>
<p>The T5 model was presented in <a class="reference external" href="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.</p>
<p>The abstract from the paper is the following,</p>
<ul>
<li><p>T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which
each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a
different prefix to the input corresponding to each task, e.g., for translation: <em>translate English to German: …</em>,
for summarization: <em>summarize: …</em>.</p>
<p>For more information about which prefix to use, it is easiest to look into Appendix D of the <a class="reference external" href="https://arxiv.org/pdf/1910.10683.pdf">paper</a>.</p>
</li>
</ul>
<div class="section" id="t5adaptermodel">
<h2>T5AdapterModel<a class="headerlink" href="#t5adaptermodel" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="adapters.T5AdapterModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">adapters.</span></span><span class="sig-name descname"><span class="pre">T5AdapterModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel" title="Permalink to this definition"></a></dt>
<dd><p>T5 Model with the option to add multiple flexible prediction heads on top.</p>
<p>The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer](<a class="reference external" href="https://arxiv.org/abs/1910.10683">https://arxiv.org/abs/1910.10683</a>) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It’s an encoder decoder transformer pre-trained in a
text-to-text denoising generative setting.</p>
<p>This model inherits from [<cite>PreTrainedModel</cite>]. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)</p>
<p>This model is also a PyTorch [torch.nn.Module](<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">https://pytorch.org/docs/stable/nn.html#torch.nn.Module</a>) subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> ([<cite>T5Config</cite>]) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the [<cite>~PreTrainedModel.from_pretrained</cite>] method to load the model weights.</p>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.active_adapters">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">active_adapters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span></em><a class="headerlink" href="#adapters.T5AdapterModel.active_adapters" title="Permalink to this definition"></a></dt>
<dd><p>If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
official documentation: <a class="reference external" href="https://huggingface.co/docs/peft">https://huggingface.co/docs/peft</a></p>
<p>Gets the current active adapters of the model. In case of multi-adapter inference (combining multiple adapters
for inference) returns the list of all active adapters so that users can deal with them accordingly.</p>
<p>For previous PEFT versions (that does not support multi-adapter inference), <cite>module.active_adapter</cite> will return
a single string.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.active_head">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">active_head</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#adapters.T5AdapterModel.active_head" title="Permalink to this definition"></a></dt>
<dd><p>The active prediction head configuration of this model. Can be either the name of a single available head
(string) or a list of multiple available heads. In case of a list of heads, the same base model is forwarded
through all specified heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A string or a list of strings describing the active head configuration.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[str, List[str]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.adapter_fusion_to">
<span class="sig-name descname"><span class="pre">adapter_fusion_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Fuse</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dtype</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.adapter_fusion_to" title="Permalink to this definition"></a></dt>
<dd><p>Moves the adapter fusion layer with the given name to the specified device and data type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_names</strong> (<em>Union</em><em>[</em><em>Fuse</em><em>, </em><em>list</em><em>, </em><em>str</em><em>]</em>) – The name of the adapter fusion layer to be moved.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em><em> or </em><em>str</em><em>, </em><em>optional</em>) – The device on which the adapter fusion layer should be moved.</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em><em>, </em><em>optional</em>) – The data type to which the adapter fusion layer should be cast.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.adapter_summary">
<span class="sig-name descname"><span class="pre">adapter_summary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">as_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.adapter_summary" title="Permalink to this definition"></a></dt>
<dd><p>Returns a string summary of all adapters currently added to the model. Each entry in the summary table has the
following attributes:</p>
<blockquote>
<div><ul class="simple">
<li><p>name: the name of the adapter</p></li>
<li><p>architecture: the architectural base of the adapter</p></li>
<li><p>#param: the number of parameters of the adapter</p></li>
<li><p>%param: the number of parameters of the adapter relative to the full model</p></li>
<li><p>active: whether the adapter is active</p></li>
<li><p>train: whether the adapter weights are enabled for training</p></li>
</ul>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.adapter_to">
<span class="sig-name descname"><span class="pre">adapter_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dtype</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.adapter_to" title="Permalink to this definition"></a></dt>
<dd><p>Moves the adapter with the given name to the specified device and data type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – The name of the adapter to be moved.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em><em> or </em><em>str</em><em>, </em><em>optional</em>) – The device on which the adapter should be moved.</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em><em>, </em><em>optional</em>) – The data type to which the adapter should be cast.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.add_adapter">
<span class="sig-name descname"><span class="pre">add_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_ok</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_active</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.add_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Adds a new adapter module of the specified type to the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_name</strong> (<em>str</em>) – The name of the adapter module to be added.</p></li>
<li><p><strong>config</strong> (<em>str</em><em> or </em><em>dict</em><em>, </em><em>optional</em>) – <p>The adapter configuration, can be either:</p>
<ul>
<li><p>the string identifier of a pre-defined configuration dictionary</p></li>
<li><p>a configuration dictionary specifying the full config</p></li>
<li><p>if not given, the default configuration for this adapter type will be used</p></li>
</ul>
</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Overwrite an adapter with the same name if it exists. By default (False), an exception is thrown.</p></li>
<li><p><strong>set_active</strong> (<em>bool</em><em>, </em><em>optional</em>) – Set the adapter to be the active one. By default (False), the adapter is added but not activated.</p></li>
</ul>
</dd>
</dl>
<p>If self.base_model is self, must inherit from a class that implements this method, to preclude infinite
recursion</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.add_adapter_fusion">
<span class="sig-name descname"><span class="pre">add_adapter_fusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Fuse</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_ok</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_active</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.add_adapter_fusion" title="Permalink to this definition"></a></dt>
<dd><p>Adds AdapterFusion to the model with alll the necessary configurations and weight initializations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_names</strong> (<em>Fuse</em><em> or </em><em>list</em><em> or </em><em>str</em>) – <p>AdapterFusion layer to add. Can be either:</p>
<ul>
<li><p>a <code class="docutils literal notranslate"><span class="pre">Fuse</span></code> composition block</p></li>
<li><p>a list of adapter names to fuse</p></li>
<li><p>a comma-separated string of adapter names to fuse</p></li>
</ul>
</p></li>
<li><p><strong>config</strong> (<em>str</em><em> or </em><em>dict</em>) – <p>adapter fusion configuration, can be either:</p>
<ul>
<li><p>a string identifying a pre-defined adapter fusion configuration</p></li>
<li><p>a dictionary representing the adapter fusion configuration</p></li>
<li><p>the path to a file containing the adapter fusion configuration</p></li>
</ul>
</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – Name of the AdapterFusion layer. If not specified, the name is generated automatically from the fused adapter names.</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Overwrite an AdapterFusion layer with the same name if it exists. By default (False), an exception is
thrown.</p></li>
<li><p><strong>set_active</strong> (<em>bool</em><em>, </em><em>optional</em>) – Activate the added AdapterFusion. By default (False), the AdapterFusion is added but not activated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.add_classification_head">
<span class="sig-name descname"><span class="pre">add_classification_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_ok</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multilabel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id2label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_pooler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.add_classification_head" title="Permalink to this definition"></a></dt>
<dd><p>Adds a sequence classification head on top of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> (<em>str</em>) – The name of the head.</p></li>
<li><p><strong>num_labels</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of classification labels. Defaults to 2.</p></li>
<li><p><strong>layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of layers. Defaults to 2.</p></li>
<li><p><strong>activation_function</strong> (<em>str</em><em>, </em><em>optional</em>) – Activation function. Defaults to ‘tanh’.</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Force overwrite if a head with the same name exists. Defaults to False.</p></li>
<li><p><strong>multilabel</strong> (<em>bool</em><em>, </em><em>optional</em>) – Enable multilabel classification setup. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.add_qa_head">
<span class="sig-name descname"><span class="pre">add_qa_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_ok</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id2label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.add_qa_head" title="Permalink to this definition"></a></dt>
<dd><p>Adds a question answering head on top of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> (<em>str</em>) – The name of the head.</p></li>
<li><p><strong>num_labels</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of classification labels. Defaults to 2.</p></li>
<li><p><strong>layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of layers. Defaults to 1.</p></li>
<li><p><strong>activation_function</strong> (<em>str</em><em>, </em><em>optional</em>) – Activation function. Defaults to ‘tanh’.</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Force overwrite if a head with the same name exists. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.add_seq2seq_lm_head">
<span class="sig-name descname"><span class="pre">add_seq2seq_lm_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_ok</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.add_seq2seq_lm_head" title="Permalink to this definition"></a></dt>
<dd><p>Adds a sequence-to-sequence language modeling head on top of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> (<em>str</em>) – The name of the head.</p></li>
<li><p><strong>layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of layers. Defaults to 1.</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Force overwrite if a head with the same name exists. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.apply_to_adapter_layers">
<span class="sig-name descname"><span class="pre">apply_to_adapter_layers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.apply_to_adapter_layers" title="Permalink to this definition"></a></dt>
<dd><p>Applies a function to all adapter layers of the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.apply_to_basemodel_childs">
<span class="sig-name descname"><span class="pre">apply_to_basemodel_childs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.apply_to_basemodel_childs" title="Permalink to this definition"></a></dt>
<dd><p>Applies a function to all direct childs of the model if they are a instance of AdapterLayerBase.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.average_adapter">
<span class="sig-name descname"><span class="pre">average_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">combine_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_ok</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_active</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">svd_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.average_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Adds a new adapter module as weighted average of a set of existing adapter modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_name</strong> (<em>str</em>) – The name of the adapter module to be added.</p></li>
<li><p><strong>adapter_list</strong> (<em>List</em><em>[</em><em>str</em><em>] or </em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>float</em><em>]</em>) – Specifies the existing adapters whose weights should be averaged. Can either be a list of adapter names
or a dictionary mapping adapter names to weights.</p></li>
<li><p><strong>weights</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>]</em><em>, </em><em>optional</em>) – The weights corresponding to each adapter module in the list.
If not provided, equal weights will be assigned to each adapter.</p></li>
<li><p><strong>combine_strategy</strong> (<em>str</em><em>, </em><em>optional</em>) – The strategy to combine the adapter modules.
Available options are “linear”, “lora_linear_only_negate_b”, and “lora_delta_w_svd”.
See <a class="reference external" href="https://docs.adapterhub.ml/adapter_composition.html#merging-adapters">https://docs.adapterhub.ml/adapter_composition.html#merging-adapters</a>
Defaults to “linear”.</p></li>
<li><p><strong>normalize_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to normalize the weights.
If True, the weights will be normalized to sum up to 1.
Defaults to True.</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Overwrite an adapter with the same name if it exists. By default (False), an exception is thrown.</p></li>
<li><p><strong>set_active</strong> (<em>bool</em><em>, </em><em>optional</em>) – Set the adapter to be the active one. By default (False), the adapter is added but not activated.</p></li>
<li><p><strong>svd_rank</strong> (<em>int</em><em>, </em><em>optional</em>) – The rank to be used for Singular Value Decomposition (SVD) when averaging LoRA adapters.
This parameter is only applicable when the combine_strategy is set to “lora_delta_w_svd”.
Defaults to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.average_head">
<span class="sig-name descname"><span class="pre">average_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_ok</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_active</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.average_head" title="Permalink to this definition"></a></dt>
<dd><p>Adds a new prediction head as a weighted average of a set of existing prediction heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> (<em>str</em>) – The name of the new prediction head to be added.</p></li>
<li><p><strong>head_list</strong> (<em>List</em><em>[</em><em>str</em><em>] or </em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>float</em><em>]</em>) – Specifies the existing heads whose weights should be averaged. Can either be a list of head names
or a dictionary mapping head names to weights.</p></li>
<li><p><strong>weights</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>]</em><em>, </em><em>optional</em>) – The weights corresponding to each head in the list.
If not provided, equal weights will be assigned to each head.</p></li>
<li><p><strong>normalize_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to normalize the weights.
If True, the weights will be normalized to sum up to 1.
Defaults to True.</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Overwrite a head with the same name if it exists. By default (False), an exception is thrown.</p></li>
<li><p><strong>set_active</strong> (<em>bool</em><em>, </em><em>optional</em>) – Set the head to be the active one. By default (False), the head is added but not activated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.delete_adapter">
<span class="sig-name descname"><span class="pre">delete_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.delete_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Deletes the adapter with the specified name from the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>adapter_name</strong> (<em>str</em>) – The name of the adapter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.delete_adapter_fusion">
<span class="sig-name descname"><span class="pre">delete_adapter_fusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Fuse</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.delete_adapter_fusion" title="Permalink to this definition"></a></dt>
<dd><p>Deletes the AdapterFusion layer of the specified adapters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>adapter_names</strong> (<em>Union</em><em>[</em><em>Fuse</em><em>, </em><em>list</em><em>, </em><em>str</em><em>]</em>) – AdapterFusion layer to delete.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.delete_head">
<span class="sig-name descname"><span class="pre">delete_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.delete_head" title="Permalink to this definition"></a></dt>
<dd><p>Deletes the prediction head with the specified name from the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>head_name</strong> (<em>str</em>) – The name of the prediction to delete.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.eject_prefix_tuning">
<span class="sig-name descname"><span class="pre">eject_prefix_tuning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.eject_prefix_tuning" title="Permalink to this definition"></a></dt>
<dd><p>Converts the prefix tuning with the given name from the reparameterized form into the flat form.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<em>str</em>) – The name of the prefix tuning.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_attn_head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_key_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.forward" title="Permalink to this definition"></a></dt>
<dd><p>The [<cite>T5AdapterModel</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>) – <p>Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you
should be able to pad the inputs on both the right and the left.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for detail.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
<p>To know more on how to prepare <cite>input_ids</cite> for pretraining take a look a [T5 Training](./t5#training).</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>decoder_input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, target_sequence_length)</cite>, <em>optional</em>) – <p>Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for details.</p>
<p>[What are decoder input IDs?](../glossary#decoder-input-ids)</p>
<p>T5 uses the <cite>pad_token_id</cite> as the starting token for <cite>decoder_input_ids</cite> generation. If <cite>past_key_values</cite>
is used, optionally only the last <cite>decoder_input_ids</cite> have to be input (see <cite>past_key_values</cite>).</p>
<p>To know more on how to prepare <cite>decoder_input_ids</cite> for pretraining take a look at [T5
Training](./t5#training).</p>
</p></li>
<li><p><strong>decoder_attention_mask</strong> (<cite>torch.BoolTensor</cite> of shape <cite>(batch_size, target_sequence_length)</cite>, <em>optional</em>) – Default behavior: generate a tensor that ignores pad tokens in <cite>decoder_input_ids</cite>. Causal mask will also
be used by default.</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules in the encoder. Mask values selected in <cite>[0,
1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>decoder_head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in <cite>[0,
1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>cross_attn_head_mask</strong> (<cite>torch.Tensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in
<cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>encoder_outputs</strong> (<cite>tuple(tuple(torch.FloatTensor)</cite>, <em>optional</em>) – Tuple consists of (<cite>last_hidden_state</cite>, <cite>optional</cite>: <em>hidden_states</em>, <cite>optional</cite>: <em>attentions</em>)
<cite>last_hidden_state</cite> of shape <cite>(batch_size, sequence_length, hidden_size)</cite> is a sequence of hidden states at
the output of the last layer of the encoder. Used in the cross-attention of the decoder.</p></li>
<li><p><strong>past_key_values</strong> (<cite>tuple(tuple(torch.FloatTensor))</cite> of length <cite>config.n_layers</cite> with each tuple having 4 tensors of shape <cite>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</cite>) – <p>Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <cite>past_key_values</cite> are used, the user can optionally input only the last <cite>decoder_input_ids</cite> (those that
don’t have their past key value states given to this model) of shape <cite>(batch_size, 1)</cite> instead of all
<cite>decoder_input_ids</cite> of shape <cite>(batch_size, sequence_length)</cite>.</p>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(batch_size, sequence_length, hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>decoder_inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(batch_size, target_sequence_length, hidden_size)</cite>, <em>optional</em>) – <p>Optionally, instead of passing <cite>decoder_input_ids</cite> you can choose to directly pass an embedded
representation. If <cite>past_key_values</cite> is used, optionally only the last <cite>decoder_inputs_embeds</cite> have to be
input (see <cite>past_key_values</cite>). This is useful if you want more control over how to convert
<cite>decoder_input_ids</cite> indices into associated vectors than the model’s internal embedding lookup matrix.</p>
<p>If <cite>decoder_input_ids</cite> and <cite>decoder_inputs_embeds</cite> are both unset, <cite>decoder_inputs_embeds</cite> takes the value
of <cite>inputs_embeds</cite>.</p>
</p></li>
<li><p><strong>use_cache</strong> (<cite>bool</cite>, <em>optional</em>) – If set to <cite>True</cite>, <cite>past_key_values</cite> key value states are returned and can be used to speed up decoding (see
<cite>past_key_values</cite>).</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p></li>
<li><p><strong>cache_position</strong> (<cite>torch.LongTensor</cite> of shape <cite>(sequence_length)</cite>, <em>optional</em>) – Indices depicting the position of the input sequence tokens in the sequence. It is used to update the
cache in the correct position and to infer the complete sequence length.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.forward_context">
<span class="sig-name descname"><span class="pre">forward_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ForwardContext</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.forward_context" title="Permalink to this definition"></a></dt>
<dd><p>This method is called by the <code class="docutils literal notranslate"><span class="pre">ForwardContext</span></code> at the beginning of the forward pass.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.forward_head">
<span class="sig-name descname"><span class="pre">forward_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">all_outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.forward_head" title="Permalink to this definition"></a></dt>
<dd><p>The forward pass through a prediction head configuration. There are three ways to specify the used prediction
head configuration (in order of priority):</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>If a head_name is passed, the head with the given name is used.</p></li>
<li><p>If the forward call is executed within an <code class="docutils literal notranslate"><span class="pre">AdapterSetup</span></code> context, the head configuration is read from
the context.</p></li>
<li><p>If the <code class="docutils literal notranslate"><span class="pre">active_head</span></code> property is set, the head configuration is read from there.</p></li>
</ol>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>all_outputs</strong> (<em>dict</em>) – The outputs of the base model.</p></li>
<li><p><strong>head_name</strong> (<em>str</em><em>, </em><em>optional</em>) – The name of the prediction head to use. If None, the active head is used.</p></li>
<li><p><strong>cls_output</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – The classification output of the model.</p></li>
<li><p><strong>attention_mask</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – The attention mask of the model.</p></li>
<li><p><strong>return_dict</strong> (<em>bool</em>) – Whether or not to return a <code class="docutils literal notranslate"><span class="pre">ModelOutput</span></code> instead of a plain tuple.</p></li>
<li><p><strong>get_cls_from_eos_tokens</strong> (<em>bool</em>) – If set to True, retrieve classifier token representations from the last &lt;eos&gt; token in the sequence.
Setting to True requires <cite>eos_mask</cite> to be passed as well.</p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments passed to the forward pass of the head.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.freeze_model">
<span class="sig-name descname"><span class="pre">freeze_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">freeze</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.freeze_model" title="Permalink to this definition"></a></dt>
<dd><p>Freezes all weights of the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.get_adapter">
<span class="sig-name descname"><span class="pre">get_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.get_adapter" title="Permalink to this definition"></a></dt>
<dd><p>If self.base_model is self, must inherit from a class that implements this method, to preclude infinite
recursion</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.get_labels">
<span class="sig-name descname"><span class="pre">get_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.get_labels" title="Permalink to this definition"></a></dt>
<dd><p>Returns the labels the given head is assigning/predictin</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> – (str, optional) the name of the head which labels should be returned. Default is None.</p></li>
<li><p><strong>returned</strong> (<em>If the name is None the labels of the active head are</em>) – </p></li>
</ul>
</dd>
</dl>
<p>Returns: labels</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.get_labels_dict">
<span class="sig-name descname"><span class="pre">get_labels_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.get_labels_dict" title="Permalink to this definition"></a></dt>
<dd><p>Returns the id2label dict for the given hea</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> – (str, optional) the name of the head which labels should be returned. Default is None.</p></li>
<li><p><strong>returned</strong> (<em>If the name is None the labels of the active head are</em>) – </p></li>
</ul>
</dd>
</dl>
<p>Returns: id2label</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.get_output_embeddings">
<span class="sig-name descname"><span class="pre">get_output_embeddings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.get_output_embeddings" title="Permalink to this definition"></a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping hidden states to vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><cite>nn.Module</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.gradient_checkpointing_enable">
<span class="sig-name descname"><span class="pre">gradient_checkpointing_enable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradient_checkpointing_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.gradient_checkpointing_enable" title="Permalink to this definition"></a></dt>
<dd><p>Activates gradient checkpointing for the current model.</p>
<p>Note that in other frameworks this feature can be referred to as “activation checkpointing” or “checkpoint
activations”.</p>
<p>We pass the <cite>__call__</cite> method of the modules instead of <cite>forward</cite> because <cite>__call__</cite> attaches all the hooks of
the module. <a class="reference external" href="https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2">https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradient_checkpointing_kwargs</strong> (dict, <em>optional</em>) – Additional keyword arguments passed along to the <cite>torch.utils.checkpoint.checkpoint</cite> function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.head_type">
<span class="sig-name descname"><span class="pre">head_type</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.head_type" title="Permalink to this definition"></a></dt>
<dd><p>Checks which head type the decorated function belongs to and raises an error if the model does not support the
head type.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.init_adapters">
<span class="sig-name descname"><span class="pre">init_adapters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapters_config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.init_adapters" title="Permalink to this definition"></a></dt>
<dd><p>This method initializes adapter modules and fusion modules from the model config.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.iter_layers">
<span class="sig-name descname"><span class="pre">iter_layers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.iter_layers" title="Permalink to this definition"></a></dt>
<dd><p>Iterates over all layers of the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.load_adapter">
<span class="sig-name descname"><span class="pre">load_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">version</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load_as</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leave_out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id2label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_active</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.load_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Loads a pre-trained pytorch adapter module from the local file system or a remote location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_name_or_path</strong> (<em>str</em>) – <p>can be either:</p>
<ul>
<li><p>the identifier of a pre-trained task adapter to be loaded from Adapter Hub</p></li>
<li><p>a path to a directory containing adapter weights saved using <cite>model.save_adapter()</cite></p></li>
<li><p>a URL pointing to a zip folder containing a saved adapter module</p></li>
</ul>
</p></li>
<li><p><strong>config</strong> (<em>dict</em><em> or </em><em>str</em><em>, </em><em>optional</em>) – Deprecated.</p></li>
<li><p><strong>version</strong> (<em>str</em><em>, </em><em>optional</em>) – The version of the adapter to be loaded.</p></li>
<li><p><strong>model_name</strong> (<em>str</em><em>, </em><em>optional</em>) – Deprecated.</p></li>
<li><p><strong>load_as</strong> (<em>str</em><em>, </em><em>optional</em>) – Load the adapter using this name. By default, the name with which the adapter was
saved will be used.</p></li>
<li><p><strong>leave_out</strong> – Dynamically drop adapter modules in the specified Transformer layers when loading the adapter.</p></li>
<li><p><strong>set_active</strong> (<em>bool</em><em>, </em><em>optional</em>) – Set the loaded adapter to be the active one. By default (False), the adapter is loaded but not
activated.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are loaded via <cite>safetensors</cite> if safetensors checkpoint is available. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The name with which the adapter was added to the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.load_adapter_fusion">
<span class="sig-name descname"><span class="pre">load_adapter_fusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_fusion_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load_as</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_active</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.load_adapter_fusion" title="Permalink to this definition"></a></dt>
<dd><p>Loads a pre-trained AdapterFusion layer from the local file system.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_fusion_name_or_path</strong> (<em>str</em>) – a path to a directory containing AdapterFusion weights saved using <cite>model.save_adapter_fusion()</cite>.</p></li>
<li><p><strong>load_as</strong> (<em>str</em><em>, </em><em>optional</em>) – Load the AdapterFusion using this name.
By default, the name with which the AdapterFusion layer was saved will be used.</p></li>
<li><p><strong>set_active</strong> (<em>bool</em><em>, </em><em>optional</em>) – Activate the loaded AdapterFusion. By default (False), the AdapterFusion is loaded but not activated.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are loaded via <cite>safetensors</cite> if safetensors checkpoint is available. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The name with which the AdapterFusion was added to the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.load_adapter_setup">
<span class="sig-name descname"><span class="pre">load_adapter_setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_setup_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">version</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_active</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.load_adapter_setup" title="Permalink to this definition"></a></dt>
<dd><p>Loads an adapter setup from the local file system or a remote location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_setup_name_or_path</strong> (<em>str</em>) – <p>can be either:</p>
<ul>
<li><p>the identifier of a repository on the HuggingFace Model Hub.</p></li>
<li><p>a path to a directory containing adapter weights saved using <cite>model.save_adapter_setup()</cite></p></li>
<li><p>a URL pointing to a zip folder containing a saved adapter module</p></li>
</ul>
</p></li>
<li><p><strong>version</strong> (<em>str</em><em>, </em><em>optional</em>) – The version of the adapter to be loaded.</p></li>
<li><p><strong>set_active</strong> (<em>bool</em><em>, </em><em>optional</em>) – Set the loaded adapter setup to be the active one. By default (False), the adapter setup is loaded but not
activated.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are loaded via <cite>safetensors</cite> if safetensors checkpoint is available. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loaded adapter setup and the head setup if available.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[AdapterCompositionBlock, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.load_head">
<span class="sig-name descname"><span class="pre">load_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load_as</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id2label</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.load_head" title="Permalink to this definition"></a></dt>
<dd><p>Loads a model prediction head from a directory where it was saved using <cite>save_head()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to the directory where the prediction head is saved.</p></li>
<li><p><strong>load_as</strong> (<em>str</em><em>, </em><em>optional</em>) – Load the AdapterFusion using this name.
By default, the name with which the AdapterFusion layer was saved will be used.</p></li>
<li><p><strong>id2label</strong> (<em>Dict</em><em>[</em><em>int</em><em>, </em><em>str</em><em>]</em><em>, </em><em>optional</em>) – Provide a custom mapping from class ids to class labels. Defaults to None.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are loaded via <cite>safetensors</cite> if safetensors checkpoint is available. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The name with which the prediction head was added to the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.merge_adapter">
<span class="sig-name descname"><span class="pre">merge_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.merge_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Merges the weights of the given LoRA module with the Transformer weights as described in the paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<em>str</em>) – LoRA module to merge.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.prepare_inputs_for_generation">
<span class="sig-name descname"><span class="pre">prepare_inputs_for_generation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_attn_head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.prepare_inputs_for_generation" title="Permalink to this definition"></a></dt>
<dd><p>Prepare the model inputs for generation. In includes operations like computing the 4D attention mask or
slicing inputs given the existing cache.</p>
<p>See the forward pass in the model documentation for expected arguments (different models might have different
requirements for e.g. <cite>past_key_values</cite>). This function should work as is for most LLMs.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.push_adapter_setup_to_hub">
<span class="sig-name descname"><span class="pre">push_adapter_setup_to_hub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repo_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_setup</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_setup</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datasets_tag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">commit_message</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">private</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_adapter_card</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_pr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">revision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">commit_description</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_card_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.push_adapter_setup_to_hub" title="Permalink to this definition"></a></dt>
<dd><p>Upload an adapter setup to HuggingFace’s Model Hub.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>repo_id</strong> (<em>str</em>) – The name of the repository on the model hub to upload to.</p></li>
<li><p><strong>adapter_setup</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>list</em><em>, </em><em>AdapterCompositionBlock</em><em>]</em>) – The adapter setup to be uploaded. Usually an adapter composition block.</p></li>
<li><p><strong>head_setup</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>bool</em><em>, </em><em>str</em><em>, </em><em>list</em><em>, </em><em>AdapterCompositionBlock</em><em>]</em><em>]</em><em>, </em><em>optional</em>) – The head setup to be uploaded.</p></li>
<li><p><strong>datasets_tag</strong> (<em>str</em><em>, </em><em>optional</em>) – Dataset identifier from <a class="reference external" href="https://huggingface.co/datasets">https://huggingface.co/datasets</a>. Defaults to
None.</p></li>
<li><p><strong>local_path</strong> (<em>str</em><em>, </em><em>optional</em>) – Local path used as clone directory of the adapter repository.
If not specified, will create a temporary directory. Defaults to None.</p></li>
<li><p><strong>commit_message</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – Message to commit while pushing. Will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;add</span> <span class="pre">config&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;add</span> <span class="pre">tokenizer&quot;</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;add</span> <span class="pre">model&quot;</span></code> depending on the type of the class.</p></li>
<li><p><strong>private</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not the repository created should be private (requires a paying subscription).</p></li>
<li><p><strong>token</strong> (<cite>bool</cite> or <cite>str</cite>, <em>optional</em>) – The token to use as HTTP bearer authorization for remote files. If <cite>True</cite>, will use the token generated
when running <cite>huggingface-cli login</cite> (stored in <cite>~/.huggingface</cite>). Will default to <cite>True</cite> if <cite>repo_url</cite>
is not specified.</p></li>
<li><p><strong>overwrite_adapter_card</strong> (<em>bool</em><em>, </em><em>optional</em>) – Overwrite an existing adapter card with a newly generated one.
If set to <cite>False</cite>, will only generate an adapter card, if none exists. Defaults to False.</p></li>
<li><p><strong>create_pr</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether or not to create a PR with the uploaded files or directly commit.</p></li>
<li><p><strong>revision</strong> (<cite>str</cite>, <em>optional</em>) – Branch to push the uploaded files to.</p></li>
<li><p><strong>commit_description</strong> (<cite>str</cite>, <em>optional</em>) – The description of the commit that will be created</p></li>
<li><p><strong>adapter_card_kwargs</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em><em>, </em><em>optional</em>) – Additional arguments to pass to the adapter card text generation.
Currently includes: tags, language, license, metrics, architecture_training, results, citation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The url of the adapter repository on the model hub.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.push_adapter_to_hub">
<span class="sig-name descname"><span class="pre">push_adapter_to_hub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repo_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datasets_tag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">commit_message</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">private</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite_adapter_card</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_pr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">revision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">commit_description</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_card_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.push_adapter_to_hub" title="Permalink to this definition"></a></dt>
<dd><p>Upload an adapter to HuggingFace’s Model Hub.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>repo_id</strong> (<em>str</em>) – The name of the repository on the model hub to upload to.</p></li>
<li><p><strong>adapter_name</strong> (<em>str</em>) – The name of the adapter to be uploaded.</p></li>
<li><p><strong>datasets_tag</strong> (<em>str</em><em>, </em><em>optional</em>) – Dataset identifier from <a class="reference external" href="https://huggingface.co/datasets">https://huggingface.co/datasets</a>. Defaults to
None.</p></li>
<li><p><strong>local_path</strong> (<em>str</em><em>, </em><em>optional</em>) – Local path used as clone directory of the adapter repository.
If not specified, will create a temporary directory. Defaults to None.</p></li>
<li><p><strong>commit_message</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – Message to commit while pushing. Will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;add</span> <span class="pre">config&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;add</span> <span class="pre">tokenizer&quot;</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;add</span> <span class="pre">model&quot;</span></code> depending on the type of the class.</p></li>
<li><p><strong>private</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not the repository created should be private (requires a paying subscription).</p></li>
<li><p><strong>token</strong> (<cite>bool</cite> or <cite>str</cite>, <em>optional</em>) – The token to use as HTTP bearer authorization for remote files. If <cite>True</cite>, will use the token generated
when running <cite>huggingface-cli login</cite> (stored in <cite>~/.huggingface</cite>). Will default to <cite>True</cite> if <cite>repo_url</cite>
is not specified.</p></li>
<li><p><strong>overwrite_adapter_card</strong> (<em>bool</em><em>, </em><em>optional</em>) – Overwrite an existing adapter card with a newly generated one.
If set to <cite>False</cite>, will only generate an adapter card, if none exists. Defaults to False.</p></li>
<li><p><strong>create_pr</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether or not to create a PR with the uploaded files or directly commit.</p></li>
<li><p><strong>revision</strong> (<cite>str</cite>, <em>optional</em>) – Branch to push the uploaded files to.</p></li>
<li><p><strong>commit_description</strong> (<cite>str</cite>, <em>optional</em>) – The description of the commit that will be created</p></li>
<li><p><strong>adapter_card_kwargs</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em><em>, </em><em>optional</em>) – Additional arguments to pass to the adapter card text generation.
Currently includes: tags, language, license, metrics, architecture_training, results, citation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The url of the adapter repository on the model hub.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.reset_adapter">
<span class="sig-name descname"><span class="pre">reset_adapter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.reset_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Resets weights of a LoRA module merged using <cite>model.merge_adapter(name)</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_adapter">
<span class="sig-name descname"><span class="pre">save_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.save_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Saves an adapter and its configuration file to a directory so that it can be shared or reloaded using
<cite>load_adapter()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the adapter should be saved.</p></li>
<li><p><strong>adapter_name</strong> (<em>str</em>) – Name of the adapter to be saved.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are saved via <cite>safetensors</cite>. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the given adapter name is invalid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_adapter_fusion">
<span class="sig-name descname"><span class="pre">save_adapter_fusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Fuse</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.save_adapter_fusion" title="Permalink to this definition"></a></dt>
<dd><p>Saves an AdapterFusion layer and its configuration file to a directory so that it can be shared or reloaded
using <cite>load_adapter_fusion()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the AdapterFusion should be saved.</p></li>
<li><p><strong>adapter_names</strong> (<em>Union</em><em>[</em><em>Fuse</em><em>, </em><em>list</em><em>, </em><em>str</em><em>]</em>) – AdapterFusion to be saved.</p></li>
<li><p><strong>with_head</strong> (<em>Union</em><em>[</em><em>bool</em><em>, </em><em>str</em><em>]</em>) – If True, will save a head with the same name as the AdapterFusionLayer. If a string, this will be used
as the name of the head to be saved.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are saved via <cite>safetensors</cite>. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the given AdapterFusion name is invalid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_adapter_setup">
<span class="sig-name descname"><span class="pre">save_adapter_setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_setup</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_setup</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.save_adapter_setup" title="Permalink to this definition"></a></dt>
<dd><p>Saves an adapter setup to a directory so that it can be shared or reloaded using <cite>load_adapter_setup()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the adapter setup should be saved.</p></li>
<li><p><strong>adapter_setup</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>list</em><em>, </em><em>AdapterCompositionBlock</em><em>]</em>) – The adapter setup to be saved. Usually an adapter composition block.</p></li>
<li><p><strong>head_setup</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>bool</em><em>, </em><em>str</em><em>, </em><em>list</em><em>, </em><em>AdapterCompositionBlock</em><em>]</em><em>]</em><em>, </em><em>optional</em>) – <p>The head setup to be saved. Can be either:</p>
<ul>
<li><p>True: save the default head for models without flex heads.</p></li>
<li><p>str: save a single head with the given name.</p></li>
<li><p>list: save a list of heads.</p></li>
<li><p>AdapterCompositionBlock: save a custom head setup.</p></li>
<li><p>None (default): do not save any heads.</p></li>
</ul>
</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are saved via <cite>safetensors</cite>. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_all_adapter_fusions">
<span class="sig-name descname"><span class="pre">save_all_adapter_fusions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.save_all_adapter_fusions" title="Permalink to this definition"></a></dt>
<dd><p>Saves all AdapterFusion layers of this model together with their configuration to subfolders of the given
location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the AdapterFusion layers should be saved.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are saved via <cite>safetensors</cite>. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_all_adapters">
<span class="sig-name descname"><span class="pre">save_all_adapters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_weights_loaders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">WeightsLoader</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.save_all_adapters" title="Permalink to this definition"></a></dt>
<dd><p>Saves all adapters of this model together with their configuration to subfolders of the given location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the adapters should be saved.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are saved via <cite>safetensors</cite>. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_all_heads">
<span class="sig-name descname"><span class="pre">save_all_heads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.save_all_heads" title="Permalink to this definition"></a></dt>
<dd><p>Saves all prediction heads of this model to subfolders of the given location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to the base directory where prediction heads should be saved.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are saved via <cite>safetensors</cite>. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_head">
<span class="sig-name descname"><span class="pre">save_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_safetensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.save_head" title="Permalink to this definition"></a></dt>
<dd><p>Saves a model prediction head to a directory such that it can be reloaded using <cite>load_head()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to the directory where the prediction head should be saved.</p></li>
<li><p><strong>head_name</strong> (<em>str</em><em>, </em><em>optional</em>) – Name of the head to save. Set to None if model only has one head. Defaults to None.</p></li>
<li><p><strong>use_safetensors</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, weights are saved via <cite>safetensors</cite>. Otherwise, the regular torch save method is used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.save_pretrained">
<span class="sig-name descname"><span class="pre">save_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PathLike</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.save_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Save a model and its configuration file to a directory, so that it can be re-loaded using the
[<cite>~PreTrainedModel.from_pretrained</cite>] class method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<cite>str</cite> or <cite>os.PathLike</cite>) – Directory to which to save. Will be created if it doesn’t exist.</p></li>
<li><p><strong>is_main_process</strong> (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>) – Whether the process calling this is the main process or not. Useful when in distributed training like
TPUs and need to call this function on all processes. In this case, set <cite>is_main_process=True</cite> only on
the main process to avoid race conditions.</p></li>
<li><p><strong>state_dict</strong> (nested dictionary of <cite>torch.Tensor</cite>) – The state dictionary of the model to save. Will default to <cite>self.state_dict()</cite>, but can be used to only
save parts of the model or if special precautions need to be taken when recovering the state dictionary
of a model (like when using model parallelism).</p></li>
<li><p><strong>save_function</strong> (<cite>Callable</cite>) – The function to use to save the state dictionary. Useful on distributed training like TPUs when one
need to replace <cite>torch.save</cite> by another method.</p></li>
<li><p><strong>push_to_hub</strong> (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>) – Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <cite>repo_id</cite> (will default to the name of <cite>save_directory</cite> in your
namespace).</p></li>
<li><p><strong>max_shard_size</strong> (<cite>int</cite> or <cite>str</cite>, <em>optional</em>, defaults to <cite>“5GB”</cite>) – <p>The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size
lower than this size. If expressed as a string, needs to be digits followed by a unit (like <cite>“5MB”</cite>).
We default it to 5GB in order for models to be able to run easily on free-tier google colab instances
without CPU OOM issues.</p>
<p>&lt;Tip warning={true}&gt;</p>
<p>If a single weight of the model is bigger than <cite>max_shard_size</cite>, it will be in its own checkpoint shard
which will be bigger than <cite>max_shard_size</cite>.</p>
<p>&lt;/Tip&gt;</p>
</p></li>
<li><p><strong>safe_serialization</strong> (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>) – Whether to save the model using <cite>safetensors</cite> or the traditional PyTorch way (that uses <cite>pickle</cite>).</p></li>
<li><p><strong>variant</strong> (<cite>str</cite>, <em>optional</em>) – If specified, weights are saved in the format pytorch_model.&lt;variant&gt;.bin.</p></li>
<li><p><strong>token</strong> (<cite>str</cite> or <cite>bool</cite>, <em>optional</em>) – The token to use as HTTP bearer authorization for remote files. If <cite>True</cite>, or not specified, will use
the token generated when running <cite>huggingface-cli login</cite> (stored in <cite>~/.huggingface</cite>).</p></li>
<li><p><strong>save_peft_format</strong> (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>) – For backward compatibility with PEFT library, in case adapter weights are attached to the model, all
keys of the state dict of adapters needs to be pre-pended with <cite>base_model.model</cite>. Advanced users can
disable this behaviours by setting <cite>save_peft_format</cite> to <cite>False</cite>.</p></li>
<li><p><strong>kwargs</strong> (<cite>Dict[str, Any]</cite>, <em>optional</em>) – Additional key word arguments passed along to the [<cite>~utils.PushToHubMixin.push_to_hub</cite>] method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.set_active_adapters">
<span class="sig-name descname"><span class="pre">set_active_adapters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_setup</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.set_active_adapters" title="Permalink to this definition"></a></dt>
<dd><p>Sets the adapter modules to be used by default in every forward pass. This setting can be overriden by passing
the <cite>adapter_names</cite> parameter in the <cite>foward()</cite> pass. If no adapter with the given name is found, no module of
the respective type will be activated. In case the calling model class supports named prediction heads, this
method will attempt to activate a prediction head with the name of the last adapter in the list of passed
adapter names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>adapter_setup</strong> (<em>list</em>) – The list of adapters to be activated by default. Can be a fusion or stacking configuration.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.share_parameters">
<span class="sig-name descname"><span class="pre">share_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">MultiTask</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reference_adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.share_parameters" title="Permalink to this definition"></a></dt>
<dd><p>Shares parameters across specified adapter layers and base model children.</p>
<p>This method enables parameter sharing between multiple adapters by linking
their parameters to a common reference. It applies the sharing operation to
both adapter layers and base model child modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_names</strong> (<em>Union</em><em>[</em><em>MultiTask</em><em>, </em><em>list</em><em>, </em><em>str</em><em>]</em>) – The names of the adapters whose
parameters should be shared. If a <cite>MultiTask</cite> object is provided, its child
adapter names will be used.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em><em>, </em><em>default=None</em>) – A custom name for the shared parameters.
If not provided, the name is derived by concatenating <cite>adapter_names</cite>.</p></li>
<li><p><strong>reference_adapter_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em><em>, </em><em>default=None</em>) – The name of an existing
adapter to use as a reference for parameter sharing.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – If any adapter configuration is not of type <cite>MultiTaskConfig</cite>.</p></li>
<li><p><strong>ValueError</strong> – If the reference adapter is not in the provided adapter names.</p></li>
<li><p><strong>AssertionError</strong> – If the adapter list is empty.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.supports_adapter">
<span class="sig-name descname"><span class="pre">supports_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">type_or_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../adapter_config.html#adapters.AdapterConfig" title="adapters.configuration.adapter_config.AdapterConfig"><span class="pre">AdapterConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#adapters.T5AdapterModel.supports_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Checks if the model supports a given adapter type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>adapter_type</strong> (<em>str</em>) – The adapter type to check.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the adapter type is supported, False otherwise.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.tie_weights">
<span class="sig-name descname"><span class="pre">tie_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.tie_weights" title="Permalink to this definition"></a></dt>
<dd><p>Tie the weights between the input embeddings and the output embeddings.</p>
<p>If the <code class="xref py py-obj docutils literal notranslate"><span class="pre">torchscript</span></code> flag is set in the configuration, can’t handle parameter sharing so we are cloning
the weights instead.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.train_adapter">
<span class="sig-name descname"><span class="pre">train_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_setup</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_embeddings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.train_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Sets the model into mode for training the given adapters. If self.base_model is self, must inherit from a class
that implements this method, to preclude infinite recursion</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.train_adapter_fusion">
<span class="sig-name descname"><span class="pre">train_adapter_fusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_setup</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">AdapterCompositionBlock</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unfreeze_adapters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.train_adapter_fusion" title="Permalink to this definition"></a></dt>
<dd><p>Sets the model into mode for training of adapter fusion determined by a list of adapter names. If
self.base_model is self, must inherit from a class that implements this method, to preclude infinite recursion</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapters.T5AdapterModel.unshare_parameters">
<span class="sig-name descname"><span class="pre">unshare_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">MultiTask</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#adapters.T5AdapterModel.unshare_parameters" title="Permalink to this definition"></a></dt>
<dd><p>Removes parameter sharing across specified adapter layers and base model children.</p>
<p>This method detaches shared parameters among the given adapters, restoring them
to independent parameter sets. The operation is applied to both adapter layers
and base model child modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_names</strong> (<em>Union</em><em>[</em><em>MultiTask</em><em>, </em><em>list</em><em>, </em><em>str</em><em>]</em>) – The names of the adapters whose
shared parameters should be unlinked. If a <cite>MultiTask</cite> object is provided,
its child adapter names will be used.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em><em>, </em><em>default=None</em>) – A custom name for the unshared parameters.
If not provided, the name is derived by concatenating <cite>adapter_names</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="roberta.html" class="btn btn-neutral float-left" title="RoBERTa" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="vit.html" class="btn btn-neutral float-right" title="Vision Transformer (ViT)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, AdapterHub Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <!--- IMPORTANT: This file has modifications compared to the snippet on the documentation page! -->
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Versions</span>
    v: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><a href="t5.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>