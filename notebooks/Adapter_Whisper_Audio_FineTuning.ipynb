{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Whisper using Adapters\n",
    "\n",
    "In this tutorial, we will be demonstrating how to fine-tune a [Whisper](https://arxiv.org/abs/2212.04356) Model using the adapters framework. We will be adding [LoRA adapters](https://docs.adapterhub.ml/methods#lora) into the Whisper Model while freezing the model weights. Then we will incorporate a sequence to sequence head on top of the model so that we can do speech recognition.\n",
    "\n",
    "For more information on the Whisper Model, please visit the huggingface model card https://huggingface.co/openai/whisper-large-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Before we can get started with the model, we need to ensure the proper packages are installed. Ensure you have `accelerate`, `bitsandbytes` and `datasets` installed along with the `adapters` library and various speech recognition libraries as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install evaluate>=0.30\n",
    "!pip install jiwer\n",
    "!pip install gradio\n",
    "!pip install -qq -U adapters accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Configuration\n",
    "\n",
    "In this tutorial, we will be using the mozilla-foundation/common_voice_11_0 dataset. In this cell, we set the proper cuda device to leverage the GPU and also set some of the dataset configurations. You can always change the below config to select what datasets you prefer the adapters model to train on. More infomation on the common voice dataset can be found [here](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA device index\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "language = \"cantonese\"\n",
    "language_abbr = \"zh-HK\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "We load the dataset and split it into its respective train and test sets. We then remove some of the columns as they are not needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\datasets\\load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\datasets\\load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 8423\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 5591\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-HK\", split=\"train\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-HK\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Feature Extractor, Tokenizer and Processor\n",
    "\n",
    "These modules are required and work specifically for the `Whisper` Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "This section is dedicated to pre-processing the dataset we loaded into something that the model can use to train and learn from. \n",
    "\n",
    "The `Whisper` model expects the sample rate to be 16000 hz, while the audio in the dataset is set at 48000 hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample down to 16000\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare a function `prepare_dataset` that will take in a batch of samples and process inputs and labels.\n",
    "\n",
    "In `prepare_dataset` we:\n",
    "1) Grab the audio data from each sample in the batch\n",
    "2) Create a new column named `input_features` that contain the extracted features when calling the `WhisperFeatureExtractor` onto the audio data\n",
    "3) Create a new column called `labels` which contain the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the built in `map' function to build our dataset using the pre-processing function before passing it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the DataCollator\n",
    "\n",
    "We now initialize a DataCollator that will be responsible for setting up a data preprocessing pipeline designed for sequence to sequence data.\n",
    "\n",
    "In the DataCollator we ensure that both the input features and our tokenized input_ids in our labels are of the same length. We do this by padding both of them to ensure they are equal, and then replace the padding values with -100 to ensure their loss values are ignored during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize our DataCollator so we can apply it to our dataset before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "We'll use the word error rate (WER) metric, a metric used primarily for evaluating performance on audio speech recognition models. For more information, please go to the WER [docs](https://huggingface.co/metrics/wer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Github\\adapters\n",
      "d:\\Documents\\Github\\adapters\\src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d:\\\\Documents\\\\Github\\\\adapters\\\\src'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some temp code to get into the right directory for imports\n",
    "import os\n",
    "%cd ..\n",
    "%cd src\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the `Whisper` adapters model\n",
    "\n",
    "Here we will initialize the adapters `Whisper` model.\n",
    "\n",
    "`adapters` acts like a wrapper to the `transformers` library so we can directly use the `WhisperConfig` for model specifications. From the `adapters` module we then import the `WhisperAdapterModel` and intialize it using the config we previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of WhisperAdapterModel were not initialized from the model checkpoint at openai/whisper-small and are newly initialized: ['heads.default.0.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperConfig\n",
    "from adapters.models.whisper.adapter_model import WhisperAdapterModel #will need replacing\n",
    "config = WhisperConfig.from_pretrained(\n",
    "    \"openai/whisper-small\",\n",
    ")\n",
    "model = WhisperAdapterModel.from_pretrained(\n",
    "    \"openai/whisper-small\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "whispter_adapter         lora             22,413,312       9.272       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               241,734,912     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#initialize the Lora Config to use as an adapter\n",
    "\n",
    "import adapters\n",
    "from adapters import LoRAConfig\n",
    "\n",
    "adapters.init(model)\n",
    "\n",
    "config = LoRAConfig(\n",
    "    selfattn_lora=True, intermediate_lora=True, output_lora=True,\n",
    "    attn_matrices=[\"q\", \"k\", \"v\"],\n",
    "    alpha=16, r=64, dropout=0.1\n",
    ")\n",
    "model.add_adapter(\"whisper_adapter\", config=config)\n",
    "model.add_seq2seq_lm_head(\"whisper_adapter\")\n",
    "model.train_adapter(\"whisper_adapter\")\n",
    "\n",
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"temp\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1, #edit this based on the number of epochs you would like to train\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=25,\n",
    "    remove_unused_columns=False, \n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AdapterTrainer\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    args=training_args,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0451f354e126439c80ce3306bde928fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Github\\adapters\\src\\adapters\\models\\whisper\\modeling_whisper.py:380: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9546, 'grad_norm': 2.2400691509246826, 'learning_rate': 0.0005, 'epoch': 0.02}\n",
      "{'loss': 0.627, 'grad_norm': 2.3768436908721924, 'learning_rate': 0.001, 'epoch': 0.05}\n",
      "{'loss': 0.7108, 'grad_norm': 2.8131179809570312, 'learning_rate': 0.0009750747756729811, 'epoch': 0.07}\n",
      "{'loss': 0.7192, 'grad_norm': 2.126743793487549, 'learning_rate': 0.0009501495513459621, 'epoch': 0.09}\n",
      "{'loss': 0.7509, 'grad_norm': 3.2347044944763184, 'learning_rate': 0.000926221335992024, 'epoch': 0.12}\n",
      "{'loss': 0.7545, 'grad_norm': 3.0252304077148438, 'learning_rate': 0.000901296111665005, 'epoch': 0.14}\n",
      "{'loss': 0.7089, 'grad_norm': 2.927863121032715, 'learning_rate': 0.0008763708873379861, 'epoch': 0.17}\n",
      "{'loss': 0.7539, 'grad_norm': 2.5152058601379395, 'learning_rate': 0.0008514456630109671, 'epoch': 0.19}\n",
      "{'loss': 0.6881, 'grad_norm': 2.572906732559204, 'learning_rate': 0.000827517447657029, 'epoch': 0.21}\n",
      "{'loss': 0.7325, 'grad_norm': 3.0144991874694824, 'learning_rate': 0.0008025922233300101, 'epoch': 0.24}\n",
      "{'loss': 0.7812, 'grad_norm': 4.132767677307129, 'learning_rate': 0.0007776669990029911, 'epoch': 0.26}\n",
      "{'loss': 0.8105, 'grad_norm': 4.141232013702393, 'learning_rate': 0.000752741774675972, 'epoch': 0.28}\n",
      "{'loss': 0.6953, 'grad_norm': 3.426758289337158, 'learning_rate': 0.0007278165503489531, 'epoch': 0.31}\n",
      "{'loss': 0.708, 'grad_norm': 2.996276617050171, 'learning_rate': 0.0007028913260219341, 'epoch': 0.33}\n",
      "{'loss': 0.6472, 'grad_norm': 2.3680903911590576, 'learning_rate': 0.0006779661016949152, 'epoch': 0.36}\n",
      "{'loss': 0.6409, 'grad_norm': 2.5756239891052246, 'learning_rate': 0.0006530408773678963, 'epoch': 0.38}\n",
      "{'loss': 0.6768, 'grad_norm': 3.384721279144287, 'learning_rate': 0.0006281156530408774, 'epoch': 0.4}\n",
      "{'loss': 0.6156, 'grad_norm': 4.924454212188721, 'learning_rate': 0.0006031904287138584, 'epoch': 0.43}\n",
      "{'loss': 0.6498, 'grad_norm': 2.4422411918640137, 'learning_rate': 0.0005782652043868395, 'epoch': 0.45}\n",
      "{'loss': 0.6014, 'grad_norm': 2.5655100345611572, 'learning_rate': 0.0005533399800598205, 'epoch': 0.47}\n",
      "{'loss': 0.5491, 'grad_norm': 2.172391653060913, 'learning_rate': 0.0005284147557328016, 'epoch': 0.5}\n",
      "{'loss': 0.5967, 'grad_norm': 2.414074659347534, 'learning_rate': 0.0005034895314057826, 'epoch': 0.52}\n",
      "{'loss': 0.5382, 'grad_norm': 1.9657586812973022, 'learning_rate': 0.0004785643070787637, 'epoch': 0.55}\n",
      "{'loss': 0.5723, 'grad_norm': 4.889124393463135, 'learning_rate': 0.00045363908275174473, 'epoch': 0.57}\n",
      "{'loss': 0.5849, 'grad_norm': 2.3223671913146973, 'learning_rate': 0.00042871385842472583, 'epoch': 0.59}\n",
      "{'loss': 0.5456, 'grad_norm': 3.0798351764678955, 'learning_rate': 0.0004037886340977069, 'epoch': 0.62}\n",
      "{'loss': 0.5609, 'grad_norm': 2.367668867111206, 'learning_rate': 0.00037886340977068793, 'epoch': 0.64}\n",
      "{'loss': 0.4812, 'grad_norm': 4.103032112121582, 'learning_rate': 0.00035393818544366903, 'epoch': 0.66}\n",
      "{'loss': 0.5779, 'grad_norm': 1.9440083503723145, 'learning_rate': 0.0003290129611166501, 'epoch': 0.69}\n",
      "{'loss': 0.5076, 'grad_norm': 1.978494644165039, 'learning_rate': 0.0003040877367896311, 'epoch': 0.71}\n",
      "{'loss': 0.457, 'grad_norm': 1.8103049993515015, 'learning_rate': 0.0002791625124626122, 'epoch': 0.74}\n",
      "{'loss': 0.4453, 'grad_norm': 1.9544771909713745, 'learning_rate': 0.0002542372881355932, 'epoch': 0.76}\n",
      "{'loss': 0.4794, 'grad_norm': 2.432069778442383, 'learning_rate': 0.00022931206380857427, 'epoch': 0.78}\n",
      "{'loss': 0.4701, 'grad_norm': 2.1191565990448, 'learning_rate': 0.00020438683948155535, 'epoch': 0.81}\n",
      "{'loss': 0.4158, 'grad_norm': 3.4953062534332275, 'learning_rate': 0.0001794616151545364, 'epoch': 0.83}\n",
      "{'loss': 0.4509, 'grad_norm': 1.940826654434204, 'learning_rate': 0.00015453639082751744, 'epoch': 0.85}\n",
      "{'loss': 0.453, 'grad_norm': 1.4921139478683472, 'learning_rate': 0.00012961116650049852, 'epoch': 0.88}\n",
      "{'loss': 0.4119, 'grad_norm': 1.9985525608062744, 'learning_rate': 0.00010468594217347957, 'epoch': 0.9}\n",
      "{'loss': 0.4524, 'grad_norm': 2.0024688243865967, 'learning_rate': 7.976071784646061e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4099, 'grad_norm': 1.9158543348312378, 'learning_rate': 5.4835493519441675e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3385, 'grad_norm': 1.582151174545288, 'learning_rate': 2.991026919242273e-05, 'epoch': 0.97}\n",
      "{'loss': 0.4315, 'grad_norm': 2.152574300765991, 'learning_rate': 4.985044865403788e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f93c6f963a4a5fa48c769f4e8646fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34885716438293457, 'eval_runtime': 4764.7537, 'eval_samples_per_second': 1.173, 'eval_steps_per_second': 0.147, 'epoch': 1.0}\n",
      "{'train_runtime': 31562.1048, 'train_samples_per_second': 0.267, 'train_steps_per_second': 0.033, 'train_loss': 0.6174209246947895, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1053, training_loss=0.6174209246947895, metrics={'train_runtime': 31562.1048, 'train_samples_per_second': 0.267, 'train_steps_per_second': 0.033, 'total_flos': 2.7032376545496e+18, 'train_loss': 0.6174209246947895, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to save your model and or publish to huggingface, sign into the huggingface_hub via the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cd1e9c42b14120bf14e465391cef72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save your model by using the `save_adapter` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the model\n",
    "save_directory = \"./my_model_directory\"\n",
    "# Save the model\n",
    "model.save_adapter(save_directory, \"whisper_adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68751275e5c14d57902933e6863171c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c42815197b4823adb8a726274e778b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model_head.bin:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5665a68c8ba9480bb7195f35b5828c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_adapter.bin:   0%|          | 0.00/89.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/julian-fong/whisper/commit/daa1db299dbf330899707acad4d36b36572a48ae', commit_message='Upload model', commit_description='', oid='daa1db299dbf330899707acad4d36b36572a48ae', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_adapter_to_hub(\n",
    "    \"whisper\",\n",
    "    \"whisper_adapter\",\n",
    "    adapterhub_tag=\"seq2seq/whisper\",\n",
    "    datasets_tag=\"mozilla-foundation/common_voice_11_0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapters_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
