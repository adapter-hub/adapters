{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Whisper using Adapters\n",
    "\n",
    "In this tutorial, we will be demonstrating how to fine-tune a [Whisper](https://arxiv.org/abs/2212.04356) model using adapters. We will be adding [LoRA](https://docs.adapterhub.ml/methods#lora) to Whisper and will incorporate a sequence to sequence head on top of the model so that we can do speech recognition. This tutorial is build on this [Whisper PEFT-Lora blog post](https://github.com/huggingface/peft/blob/main/examples/int8_training/peft_bnb_whisper_large_v2_training.ipynb). \n",
    "\n",
    "For more information on the Whisper Model, please visit the [Hugging Face model card](https://huggingface.co/openai/whisper-large-v3) or see the original [OpenAI Blog Post](https://openai.com/index/whisper/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Before we can get started with the model, we need to ensure the proper packages are installed. Ensure you have `accelerate`, `bitsandbytes` and `datasets` installed along with the `adapters` library and various speech recognition libraries as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from librosa) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\jackson\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Using cached numba-0.60.0-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Using cached soxr-0.4.0-cp312-cp312-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Collecting lazy-loader>=0.1 (from librosa)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Using cached msgpack-1.0.8-cp312-cp312-win_amd64.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Using cached llvmlite-0.43.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.7.4)\n",
      "Using cached librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached msgpack-1.0.8-cp312-cp312-win_amd64.whl (75 kB)\n",
      "Using cached numba-0.60.0-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Using cached soxr-0.4.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached llvmlite-0.43.0-cp312-cp312-win_amd64.whl (28.1 MB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: soxr, pycparser, msgpack, llvmlite, lazy-loader, audioread, pooch, numba, cffi, soundfile, librosa\n",
      "Successfully installed audioread-3.0.1 cffi-1.16.0 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.43.0 msgpack-1.0.8 numba-0.60.0 pooch-1.8.2 pycparser-2.22 soundfile-0.12.1 soxr-0.4.0\n",
      "Collecting jiwer\n",
      "  Using cached jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\jackson\\appdata\\roaming\\python\\python312\\site-packages (from jiwer) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
      "  Using cached rapidfuzz-3.9.5-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Using cached jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
      "Using cached rapidfuzz-3.9.5-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Installing collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.4 rapidfuzz-3.9.5\n",
      "Collecting gradio\n",
      "  Using cached gradio-4.40.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting fastapi (from gradio)\n",
      "  Downloading fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.2.0 (from gradio)\n",
      "  Using cached gradio_client-1.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (0.24.5)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio)\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (2.11.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (2.0.1)\n",
      "Collecting matplotlib~=3.0 (from gradio)\n",
      "  Using cached matplotlib-3.9.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Using cached orjson-3.10.6-cp312-none-win_amd64.whl.metadata (51 kB)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (10.2.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.5.6-py3-none-win_amd64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from gradio-client==1.2.0->gradio) (2024.2.0)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.2.0->gradio)\n",
      "  Using cached websockets-12.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio)\n",
      "  Using cached contourpy-1.2.1-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio)\n",
      "  Using cached fonttools-4.53.1-cp312-cp312-win_amd64.whl.metadata (165 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=2.0->gradio)\n",
      "  Using cached pydantic_core-2.20.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\envs\\adapter_hub\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Using cached gradio-4.40.0-py3-none-any.whl (12.5 MB)\n",
      "Using cached gradio_client-1.2.0-py3-none-any.whl (318 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached matplotlib-3.9.1-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "Using cached orjson-3.10.6-cp312-none-win_amd64.whl (136 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading ruff-0.5.6-py3-none-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/8.7 MB 3.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/8.7 MB 3.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/8.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.9/8.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.1/8.7 MB 5.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.4/8.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.6/8.7 MB 5.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/8.7 MB 5.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/8.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/8.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.8/8.7 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.1/8.7 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.3/8.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.6/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.9/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.2/8.7 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.4/8.7 MB 5.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.6/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.9/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.2/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.4/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.7/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.0/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.2/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.4/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.7/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.9/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.4/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.6/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.9/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.1/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.1/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.2/8.7 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.4/8.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 5.0 MB/s eta 0:00:00\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Downloading uvicorn-0.30.5-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.8/62.8 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
      "   ---------------------------------------- 0.0/93.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 93.1/93.1 kB 2.6 MB/s eta 0:00:00\n",
      "Using cached ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached contourpy-1.2.1-cp312-cp312-win_amd64.whl (189 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.53.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl (56 kB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached websockets-12.0-cp312-cp312-win_amd64.whl (124 kB)\n",
      "Installing collected packages: pydub, websockets, tomlkit, sniffio, shellingham, semantic-version, ruff, python-multipart, pyparsing, pydantic-core, orjson, kiwisolver, importlib-resources, h11, fonttools, ffmpy, cycler, contourpy, annotated-types, aiofiles, uvicorn, rich, pydantic, matplotlib, httpcore, anyio, typer, starlette, httpx, gradio-client, fastapi, gradio\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 anyio-4.4.0 contourpy-1.2.1 cycler-0.12.1 fastapi-0.112.0 ffmpy-0.4.0 fonttools-4.53.1 gradio-4.40.0 gradio-client-1.2.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 importlib-resources-6.4.0 kiwisolver-1.4.5 matplotlib-3.9.1 orjson-3.10.6 pydantic-2.8.2 pydantic-core-2.20.1 pydub-0.25.1 pyparsing-3.1.2 python-multipart-0.0.9 rich-13.7.1 ruff-0.5.6 semantic-version-2.10.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 uvicorn-0.30.5 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "!pip install evaluate>=0.30\n",
    "!pip install jiwer\n",
    "!pip install gradio\n",
    "!pip install -qq -U adapters accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Configuration\n",
    "\n",
    "In this tutorial, we will be using the mozilla-foundation/common_voice_11_0 dataset. In this cell, we set the proper cuda device to leverage the GPU and also set some of the dataset configurations. You can always change the below config to select what datasets you prefer the adapters model to train on. More infomation on the common voice dataset can be found [here](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA device index\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name_or_path = \"openai/whisper-tiny\"\n",
    "language = \"cantonese\"\n",
    "language_abbr = \"zh-HK\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "We load the dataset and split it into its respective train and test sets. We then remove some of the columns as they are not needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapter_hub\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Anaconda\\envs\\adapter_hub\\Lib\\site-packages\\datasets\\load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\adapter_hub\\Lib\\site-packages\\datasets\\load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 8423\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 5591\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-HK\", split=\"train\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-HK\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\Jackson\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\38272d4d8a5becb490327bdb81aef3d7a11ae9499ba16e02965a27567411ad93\\\\zh-HK_train_0/common_voice_zh-HK_22942304.mp3', 'array': array([ 0.00000000e+00,  4.30017540e-13,  6.87821111e-13, ...,\n",
      "       -1.39293297e-06, -6.22257721e-06, -1.14162267e-05]), 'sampling_rate': 48000}, 'sentence': 'ÊâçËÉΩÂãáÂæÄÁõ¥Ââç'}\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "These modules are required for data processing and work specifically for the `Whisper` Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This section is dedicated to pre-processing the dataset we loaded into something that the model can use to train and learn from. \n",
    "\n",
    "The `Whisper` model expects the sample rate to be 16000 hz, while the audio in the dataset is set at 48000 hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample down to 16000\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\Jackson\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\38272d4d8a5becb490327bdb81aef3d7a11ae9499ba16e02965a27567411ad93\\\\zh-HK_train_0/common_voice_zh-HK_22942304.mp3', 'array': array([ 5.45696821e-12,  2.72848411e-12,  3.63797881e-12, ...,\n",
      "        1.48210138e-05,  9.73203896e-07, -4.09249424e-06]), 'sampling_rate': 16000}, 'sentence': 'ÊâçËÉΩÂãáÂæÄÁõ¥Ââç'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare a function `prepare_dataset` that will take in a batch of samples and process inputs and labels.\n",
    "\n",
    "In `prepare_dataset` we:\n",
    "1) Grab the audio data from each sample in the batch\n",
    "2) Create a new column named `input_features` that contain the extracted features when calling the `WhisperFeatureExtractor` onto the audio data\n",
    "3) Create a new column called `labels` which contain the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the built in `map' function to build our dataset using the pre-processing function before passing it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8423/8423 [04:02<00:00, 34.80 examples/s]  \n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5591/5591 [02:23<00:00, 39.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the DataCollator\n",
    "\n",
    "We now define a DataCollator class that will be responsible for batching and preprocessing our speech-to-text data.\n",
    "\n",
    "In the DataCollator we ensure that both the input features and our tokenized input_ids in our labels are of the same length. We do this by padding both of them to ensure they are equal, and then replace the padding values with -100 to ensure their loss values are ignored during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize our DataCollator so we can apply it to our dataset during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "We'll use the word error rate (WER) metric, a metric used primarily for evaluating performance on audio speech recognition models. For more information, please go to the WER [docs](https://huggingface.co/metrics/wer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the `Whisper` adapters model\n",
    "\n",
    "Here we will setup the adapters `Whisper` model. You can see that the code used to initialize the model looks somewhat identical to code used inside `HuggingFace`. This is because `adapters` acts like a wrapper to the `HuggingFace` library, so users familiar with `HuggingFace` can easily integrate their own code while combining it with `adapters` code. The cool thing here is that we can load the `WhisperConfig` directly from `transformers` and use it for our `Whisper` model specifications. \n",
    "\n",
    "From the `adapters` module we then import the `WhisperAdapterModel` and intialize it using the config we previously imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperConfig\n",
    "from adapters import WhisperAdapterModel\n",
    "\n",
    "config = WhisperConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")\n",
    "model = WhisperAdapterModel.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add an untrained adapter named `whisper_adapter` that we will fine-tune instead of the ``Whisper`` model parameters. Afterwards we include a sequence to sequence language modeling head so we can generate tokens from the ``Whisper`` model.\n",
    "\n",
    "Once these two components are added, we leverage the `train_adapter` function to make sure `adapters` knows what adapter weights that need to be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "whisper_adapter          lora              3,735,552       9.893       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                                37,760,640     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import adapters\n",
    "#initialize the Lora Config to use as an adapter\n",
    "from adapters import LoRAConfig\n",
    "\n",
    "\n",
    "config = LoRAConfig(\n",
    "    selfattn_lora=True, intermediate_lora=True, output_lora=True,\n",
    "    attn_matrices=[\"q\", \"k\", \"v\"],\n",
    "    alpha=16, r=64, dropout=0.1\n",
    ")\n",
    "model.add_adapter(\"whisper_adapter\", config=config)\n",
    "model.add_seq2seq_lm_head(\"whisper_adapter\")\n",
    "model.train_adapter(\"whisper_adapter\")\n",
    "\n",
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapter_hub\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initalize training arguments directly from HuggingFace\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"temp\",  # change to a directory name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1, #edit this based on the number of epochs you would like to train\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=25,\n",
    "    remove_unused_columns=False, \n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = common_voice[\"train\"].select(range(500))\n",
    "subset2 = common_voice[\"test\"].select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the `adapters` sequence to sequence adapter trainer. This works very similarly to the `Trainer` module inside `Huggingface`, saving you the trouble from needing to write any extra code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapter_hub\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from adapters import Seq2SeqAdapterTrainer\n",
    "\n",
    "trainer = Seq2SeqAdapterTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=subset1,\n",
    "    eval_dataset=subset2,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]D:\\Documents\\Github\\adapters\\src\\adapters\\models\\whisper\\modeling_whisper.py:380: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:57<01:12,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2212, 'grad_norm': 3.6394524574279785, 'learning_rate': 0.00044, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [01:45<00:25,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1574, 'grad_norm': 3.3313405513763428, 'learning_rate': 0.00094, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [02:30<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3545207977294922, 'eval_runtime': 18.6904, 'eval_samples_per_second': 5.35, 'eval_steps_per_second': 0.696, 'epoch': 1.0}\n",
      "{'train_runtime': 150.2277, 'train_samples_per_second': 3.328, 'train_steps_per_second': 0.419, 'train_loss': 1.980199102371458, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=1.980199102371458, metrics={'train_runtime': 150.2277, 'train_samples_per_second': 3.328, 'train_steps_per_second': 0.419, 'total_flos': 1.499904e+16, 'train_loss': 1.980199102371458, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference\n",
    "\n",
    "We can use the below cell to see how well our fine-tuned `Whisper` model performs. We use the WER metric we initialized earlier to mark the performance of the model. To learn more about WER, you can visit this [link](https://huggingface.co/spaces/evaluate-metric/wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]C:\\Users\\Jackson\\AppData\\Local\\Temp\\ipykernel_31740\\4242628702.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "d:\\Anaconda\\envs\\adapter_hub\\Lib\\site-packages\\transformers\\generation\\utils.py:941: FutureWarning: You have explicitly specified `forced_decoder_ids`. This functionality has been deprecated and will throw an error in v4.40. Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively.\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [01:04<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=98.0\n"
     ]
    }
   ],
   "source": [
    "#model inference\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "eval_dataloader = DataLoader(subset2, batch_size=8, collate_fn=data_collator)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(device),\n",
    "                    decoder_input_ids=batch[\"labels\"][:, :4].to(device),\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            metric.add_batch(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "            )\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * metric.compute()\n",
    "print(f\"{wer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to save your model and or publish to huggingface, sign into the huggingface_hub via the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always save your model by using the `save_adapter` function locally onto your computer or you can directly upload them to the ``HuggingFace`` hub. Make sure you include the parameter `adapterhub_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the model\n",
    "save_directory = \"./my_model_directory\"\n",
    "# Save the model\n",
    "model.save_adapter(save_directory, \"whisper_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_adapter_to_hub(\n",
    "    \"whisper\",\n",
    "    \"whisper_adapter\",\n",
    "    datasets_tag=\"mozilla-foundation/common_voice_11_0\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapters_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
