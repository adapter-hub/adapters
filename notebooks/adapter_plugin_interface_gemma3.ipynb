{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abea4469",
   "metadata": {},
   "source": [
    "# Adding Adapter Support for Gemma 3 with Plugin Interface\n",
    "\n",
    "This notebook demonstrates how to add adapter support to the Gemma 3 model using the adapters library's plugin interface. Adapters are a parameter-efficient fine-tuning technique that allows you to adapt pre-trained language models to new tasks while only training a small number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9334584",
   "metadata": {},
   "source": [
    "## 1. Installing Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the adapters library and transformers\n",
    "!pip install -q adapters transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd4728",
   "metadata": {},
   "source": [
    "## 2. Understanding the Model Architecture\n",
    "\n",
    "Before creating our plugin interface, let's understand the basic structure of Gemma 3:\n",
    "\n",
    "- Like most Transformer language models, it consists of an embedding layer followed by a series of decoder layers\n",
    "- Each layer contains a self-attention block and an MLP block\n",
    "- The self-attention block includes query, key, value, and output projections\n",
    "- The MLP block includes multiple linear projections\n",
    "\n",
    "To create an adapter interface, we need to map these components to the appropriate adapter hooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a760598",
   "metadata": {},
   "source": [
    "## 3. Creating the Plugin Interface\n",
    "\n",
    "Now we'll create a plugin interface for Gemma 3 that maps the model's architecture to the adapter framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71f2ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapters\n",
    "from adapters import AdapterModelInterface\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "plugin_interface = AdapterModelInterface(\n",
    "    # Specify which adapter methods to enable\n",
    "    adapter_methods=[\"lora\", \"reft\"],\n",
    "    \n",
    "    # Map the model's components to the adapter interface\n",
    "    model_embeddings=\"embed_tokens\",      # Embedding layer\n",
    "    model_layers=\"layers\",                # Transformer layers\n",
    "    layer_self_attn=\"self_attn\",          # Self-attention module in each layer\n",
    "    layer_cross_attn=None,                # Gemma 3 doesn't have cross-attention\n",
    "    \n",
    "    # Projection matrices within the attention module\n",
    "    attn_k_proj=\"k_proj\",                 # Key projection\n",
    "    attn_q_proj=\"q_proj\",                 # Query projection\n",
    "    attn_v_proj=\"v_proj\",                 # Value projection\n",
    "    attn_o_proj=\"o_proj\",                 # Output projection\n",
    "    \n",
    "    # MLP projections\n",
    "    layer_intermediate_proj=\"mlp.up_proj\",  # Upward projection in MLP\n",
    "    layer_output_proj=\"mlp.down_proj\",      # Downward projection in MLP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5be9c",
   "metadata": {},
   "source": [
    "Each parameter in the interface maps to specific module names in the model's architecture, allowing the adapter methods to hook into the right components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a555a4",
   "metadata": {},
   "source": [
    "## 4. Loading the Model and Initializing with the Interface\n",
    "\n",
    "Now, let's load the Gemma 3 model and initialize it with our plugin interface.\n",
    "\n",
    "⚠️ Note: Gemma 3 is a gated model that requires a HuggingFace token for access. Make sure you have accepted the model terms on the HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff401e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Import HuggingFace token\n",
    "import os\n",
    "# Set your HuggingFace token here, or set it as an environment variable\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"<YOUR_TOKEN>\" \n",
    "\n",
    "# For demonstration purposes, we'll use a smaller version of the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-it\",  # You can switch to google/gemma-3-8b if you have enough resources\n",
    "    token=os.environ.get(\"HUGGINGFACE_TOKEN\"),  # Required for gated models\n",
    "    device_map=\"auto\"  # Automatically distribute model across available GPUs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79829238",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'functools.partial' object has no attribute '__func__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the adapter framework with our plugin interface\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43madapters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugin_interface\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\src\\adapters\\wrappers\\model.py:134\u001b[0m, in \u001b[0;36minit\u001b[1;34m(model, adapters_config, interface)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, ModelAdaptersMixin):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(base_model, ModelAdaptersMixin):\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;66;03m# HACK to preserve original forward method signature (e.g. for Trainer label names)\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m         temp_signature \u001b[38;5;241m=\u001b[39m ForwardContext\u001b[38;5;241m.\u001b[39madd_context_args_in_signature(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__func__\u001b[39;49m)\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;66;03m# Create new wrapper model class\u001b[39;00m\n\u001b[0;32m    136\u001b[0m         model_class_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'functools.partial' object has no attribute '__func__'"
     ]
    }
   ],
   "source": [
    "# Initialize the adapter framework with our plugin interface\n",
    "adapters.init(model, interface=plugin_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b508c",
   "metadata": {},
   "source": [
    "## 5. Adding and Training an Adapter\n",
    "\n",
    "With the interface in place, we can now add an adapter to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "gemma3-finance-adapter   lora              1,490,944       0.149       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               999,885,952     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from adapters import LoRAConfig\n",
    "\n",
    "# Add a LoRA adapter\n",
    "adapter_name = \"gemma3-math-adapter\"\n",
    "lora_config = LoRAConfig(\n",
    "    r=16,           # LoRA rank\n",
    "    alpha=32,       # LoRA alpha parameter\n",
    "    dropout=0.05,   # Dropout probability for LoRA layers\n",
    ")\n",
    "\n",
    "# model.add_adapter(adapter_name, config=lora_config)\n",
    "\n",
    "# Activate the adapter\n",
    "model.set_active_adapters(adapter_name)\n",
    "\n",
    "# Set the model to train only the adapter parameters\n",
    "model.train_adapter(adapter_name)\n",
    "\n",
    "# Verify adapter was correctly added\n",
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0129bb8",
   "metadata": {},
   "source": [
    "## 6. Loading the GSM8K Dataset for Fine-tuning\n",
    "\n",
    "For this example, we'll use the GSM8K dataset to fine-tune our model for solving grade school math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 2264\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "print(dataset)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'label': 1}\n",
      "Label distribution: ['negative', 'neutral', 'positive']\n",
      "ClassLabel(names=['negative', 'neutral', 'positive'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# Explore sample data\n",
    "print(\"Sample question:\")\n",
    "print(dataset[\"train\"][0][\"question\"])\n",
    "print(\"\\nSample answer:\")\n",
    "print(dataset[\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f7c79",
   "metadata": {},
   "source": [
    "## 7. Preprocessing the Dataset\n",
    "\n",
    "We need to tokenize our math problems and their solutions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2264/2264 [00:00<00:00, 7365.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Create full prompts with question and expected answer format\n",
    "    prompts = [\n",
    "        f\"Solve the following math problem step-by-step:\\n\\nQuestion: {q}\\n\\nAnswer: {a}\" \n",
    "        for q, a in zip(examples[\"question\"], examples[\"answer\"])\n",
    "    ]\n",
    "    \n",
    "    # Tokenize as regular sequences\n",
    "    tokenized = tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=768)\n",
    "    \n",
    "    # For causal language modeling, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])\n",
    "\n",
    "print(\"Dataset processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dabf9",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning the Adapter\n",
    "\n",
    "Now we can fine-tune our adapter for solving math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30339714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "# For math problem solving, we'll use perplexity as our main metric\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    # Shift labels to align with predictions (standard for causal language modeling)\n",
    "    shifted_logits = logits[..., :-1, :].contiguous()\n",
    "    shifted_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1))\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return {\"perplexity\": perplexity, \"loss\": loss.item()}\n",
    "\n",
    "# Set up training arguments - adjusted for math problem solving\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma3-math-adapter\",\n",
    "    per_device_train_batch_size=2,  # Smaller batch size due to longer sequences\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,  # More epochs for complex task\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",  # Use loss as metric for best model\n",
    "    greater_is_better=False,  # Lower loss is better\n",
    "    push_to_hub=False,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch sizes\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if available\n",
    "    warmup_ratio=0.1,  # Add some warmup steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1448\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 363\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and validation\n",
    "# Use a smaller subset for faster training if needed\n",
    "train_dataset = tokenized_dataset[\"train\"].select(range(min(len(tokenized_dataset[\"train\"]), 2000)))\n",
    "eval_dataset = tokenized_dataset[\"test\"].select(range(min(len(tokenized_dataset[\"test\"]), 200)))\n",
    "\n",
    "print(f\"Training on {len(train_dataset)} examples and evaluating on {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c50119",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (512) to match target batch_size (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m AdapterTrainer(\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      6\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train only the adapter parameters\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2554\u001b[0m )\n\u001b[0;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2562\u001b[0m ):\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\trainer.py:3718\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3717\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3718\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3720\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3723\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3724\u001b[0m ):\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\trainer.py:3783\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3781\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3782\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3783\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3785\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Clifton\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Clifton\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\src\\adapters\\context.py:182\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapters_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m--> 182\u001b[0m         results \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39m_call_forward(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\src\\adapters\\context.py:115\u001b[0m, in \u001b[0;36mForwardContext._call_forward\u001b[1;34m(self, model, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03mCalls the forward function of the model with the given arguments and keyword arguments.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_args}\n\u001b[1;32m--> 115\u001b[0m results \u001b[38;5;241m=\u001b[39m f(model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# append output attributes\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\src\\adapters\\model_mixin.py:2570\u001b[0m, in \u001b[0;36mModelWithHeadsAdaptersMixin.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2568\u001b[0m \u001b[38;5;129m@ForwardContext\u001b[39m\u001b[38;5;241m.\u001b[39mwrap\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\models\\gemma3\\modeling_gemma3.py:1003\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1003\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(logits, labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1006\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\loss\\loss_utils.py:56\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[1;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[0;32m     55\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 56\u001b[0m loss \u001b[38;5;241m=\u001b[39m fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\hf_transformers\\src\\transformers\\loss\\loss_utils.py:27\u001b[0m, in \u001b[0;36mfixed_cross_entropy\u001b[1;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfixed_cross_entropy\u001b[39m(source, target, num_items_in_batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     29\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m num_items_in_batch\n",
      "File \u001b[1;32mc:\\Users\\Clifton\\Dokumente\\Uni\\nlp\\selfcontained-adapters-dev\\env\\lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (512) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "from adapters import AdapterTrainer\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train only the adapter parameters\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4b99b",
   "metadata": {},
   "source": [
    "## 9. Saving and Loading the Adapter\n",
    "\n",
    "After training, we can save just the adapter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ef8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the adapter weights\n",
    "model.save_adapter(\"./gemma3-math-adapter\", adapter_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14290123",
   "metadata": {},
   "source": [
    "## 10. Testing the Adapter\n",
    "\n",
    "Let's test our math problem-solving adapter on some new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0380a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the model with a few math problems\n",
    "test_examples = [\n",
    "    \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four eggs. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "    \"John has 5 pens, 2 pencils, and 3 erasers in his drawer. If he randomly picks 3 items from the drawer, what is the probability that he picks exactly 2 pens?\",\n",
    "    \"A rectangle has a length of 12 cm and a width of 8 cm. What is its area in square centimeters?\"\n",
    "]\n",
    "\n",
    "# Format the test examples with the prompt template\n",
    "test_prompts = [\n",
    "    f\"Solve the following math problem step-by-step:\\n\\nQuestion: {text}\\n\\nAnswer:\" \n",
    "    for text in test_examples\n",
    "]\n",
    "\n",
    "# Tokenize the test prompts\n",
    "test_inputs = tokenizer(test_prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "# Generate responses\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=300,  # More tokens for step-by-step solutions\n",
    "        temperature=0.3,     # Lower temperature for more deterministic answers\n",
    "        do_sample=True,      # Some sampling for creativity in problem-solving\n",
    "        num_beams=3,         # Beam search for better coherence\n",
    "        no_repeat_ngram_size=2  # Avoid repetition\n",
    "    )\n",
    "\n",
    "# Decode and print the results\n",
    "for i, output in enumerate(outputs):\n",
    "    generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"Problem: {test_examples[i]}\")\n",
    "    print(f\"Solution:\\n{generated_text}\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code demonstrates how you would reload the model and adapter in a new session\n",
    "# We're not executing this in the notebook as we already have the model loaded\n",
    "\n",
    "'''\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-it\", \n",
    "    token=\"YOUR_HF_TOKEN\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create the plugin interface again\n",
    "plugin_interface = AdapterModelInterface(\n",
    "    adapter_methods=[\"lora\", \"reft\"],\n",
    "    model_embeddings=\"embed_tokens\",\n",
    "    model_layers=\"layers\",\n",
    "    layer_self_attn=\"self_attn\",\n",
    "    layer_cross_attn=None,\n",
    "    attn_k_proj=\"k_proj\",\n",
    "    attn_q_proj=\"q_proj\",\n",
    "    attn_v_proj=\"v_proj\",\n",
    "    attn_o_proj=\"o_proj\",\n",
    "    layer_intermediate_proj=\"mlp.up_proj\",\n",
    "    layer_output_proj=\"mlp.down_proj\",\n",
    ")\n",
    "\n",
    "# Initialize adapter support\n",
    "adapters.init(model, interface=plugin_interface)\n",
    "\n",
    "# Load the adapter\n",
    "model.load_adapter(\"./gemma3-math-adapter\", adapter_name=\"math\")\n",
    "\n",
    "# Activate the adapter\n",
    "model.set_active_adapters(\"math\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8b45b",
   "metadata": {},
   "source": [
    "## 12. Note on HybridCache for Gemma 3\n",
    "\n",
    "One thing to note about Gemma 3 is that it uses a special \"HybridCache\" for attention. This is different from standard key-value caching mechanisms and requires special handling in some generation scenarios. However, for adapter training, we don't need to worry about this since we're only modifying specific components and not changing the caching mechanism.\n",
    "\n",
    "The HybridCache implementation in Gemma 3 is used for efficient generation and doesn't interfere with adapter training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d474e9",
   "metadata": {},
   "source": [
    "## 13. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Create a plugin interface for adding adapter support to Gemma 3\n",
    "2. Load and initialize the model with the adapter framework\n",
    "3. Add a LoRA adapter to the model\n",
    "4. Fine-tune the adapter on the GSM8K math problem-solving task\n",
    "5. Save and reload the adapter weights\n",
    "6. Use the adapter for solving new math problems\n",
    "\n",
    "The plugin interface approach allows you to use parameter-efficient fine-tuning with any Transformer model, even those not officially supported by the adapters library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
