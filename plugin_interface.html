<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Models &mdash; AdapterHub  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css" />

  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ALBERT" href="classes/models/albert.html" />
    <link rel="prev" title="Model Overview" href="model_overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            AdapterHub
              <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="transitioning.html">Transitioning from <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Adapter Methods</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview and Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="methods.html">Adapter Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="method_combinations.html">Method Combinations</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_task_methods.html">Multi Task Methods</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adapter_composition.html">Adapter Activation and Composition</a></li>
<li class="toctree-l1"><a class="reference internal" href="merging_adapters.html">Merging Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="prediction_heads.html">Prediction Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">Embeddings</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Loading and Sharing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="huggingface_hub.html">Integration with Hugging Face’s Model Hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="model_overview.html">Model Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Custom Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pre-supported-models">Pre-supported Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adding-support-for-new-models">Adding Support for New Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#contributing-interfaces">Contributing Interfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="#walkthrough">Walkthrough</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/beit.html">BEiT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert-generation.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/clip.html">CLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/deberta_v2.html">DeBERTa-v2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gptj.html">EleutherAI GPT-J-6B</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/llama.html">LLaMA</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/mistral.html">Mistral</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/plbart.html">PLBART</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/vit.html">Vision Transformer (ViT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/whisper.html">Whisper</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/xmod.html">X-MOD</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_layer.html">Adapter Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_model_interface.html">Adapter Model Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_utils.html">Adapter Utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to AdapterHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing/adding_adapter_methods.html">Adding Adapter Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing/adding_adapters_to_a_model.html">Adding Adapters to a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending the Library</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AdapterHub</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Custom Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/plugin_interface.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="custom-models">
<h1>Custom Models<a class="headerlink" href="#custom-models" title="Permalink to this heading"></a></h1>
<p>The <em>Adapters</em> library provides a simple mechanism for integrating adapter methods into any available <em>Transformers</em> model - including custom architectures. While most adapter methods are supported through this interface, some features like Prefix Tuning are not available.</p>
<div class="section" id="pre-supported-models">
<h2>Pre-supported Models<a class="headerlink" href="#pre-supported-models" title="Permalink to this heading"></a></h2>
<p>Some models already have interfaces provided by default in the library. For these models, you can simply initialize the model using adapters without specifying an interface:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">adapters</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForMaskedLM</span> 

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;answerdotai/ModernBERT-base&quot;</span><span class="p">)</span>  
<span class="n">adapters</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Check out our <a class="reference internal" href="#model_overview.html"><span class="xref myst">Model Overview</span></a> page to see all models that are supported out of the box.</p>
</div>
<div class="section" id="adding-support-for-new-models">
<h2>Adding Support for New Models<a class="headerlink" href="#adding-support-for-new-models" title="Permalink to this heading"></a></h2>
<p>If we don’t support your model yet, you can easily add adapter support by defining a plugin interface instance of <a class="reference internal" href="classes/adapter_model_interface.html#adapters.AdapterModelInterface" title="adapters.AdapterModelInterface"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterModelInterface</span></code></span></a>. Here’s an example for Gemma 2:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">adapters</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdapterModelInterface</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">plugin_interface</span> <span class="o">=</span> <span class="n">AdapterModelInterface</span><span class="p">(</span>
    <span class="n">adapter_methods</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;lora&quot;</span><span class="p">,</span> <span class="s2">&quot;reft&quot;</span><span class="p">],</span>
    <span class="n">model_embeddings</span><span class="o">=</span><span class="s2">&quot;embed_tokens&quot;</span><span class="p">,</span>
    <span class="n">model_layers</span><span class="o">=</span><span class="s2">&quot;layers&quot;</span><span class="p">,</span>
    <span class="n">layer_self_attn</span><span class="o">=</span><span class="s2">&quot;self_attn&quot;</span><span class="p">,</span>
    <span class="n">layer_cross_attn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">attn_k_proj</span><span class="o">=</span><span class="s2">&quot;k_proj&quot;</span><span class="p">,</span>
    <span class="n">attn_q_proj</span><span class="o">=</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span>
    <span class="n">attn_v_proj</span><span class="o">=</span><span class="s2">&quot;v_proj&quot;</span><span class="p">,</span>
    <span class="n">attn_o_proj</span><span class="o">=</span><span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
    <span class="n">layer_intermediate_proj</span><span class="o">=</span><span class="s2">&quot;mlp.up_proj&quot;</span><span class="p">,</span>
    <span class="n">layer_output_proj</span><span class="o">=</span><span class="s2">&quot;mlp.down_proj&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/gemma-2-2b-it&quot;</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="s2">&quot;&lt;YOUR_TOKEN&gt;&quot;</span><span class="p">)</span>
<span class="n">adapters</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="n">plugin_interface</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;my_adapter&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="s2">&quot;lora&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">adapter_summary</span><span class="p">())</span>
</pre></div>
</div>
<div class="section" id="contributing-interfaces">
<h3>Contributing Interfaces<a class="headerlink" href="#contributing-interfaces" title="Permalink to this heading"></a></h3>
<p>We encourage you to share your adapter interfaces with the community! The interfaces for all pre-supported models can be found in our <a class="reference external" href="https://github.com/adapter-hub/adapters/blob/main/src/adapters/wrappers/interfaces.py"><code class="docutils literal notranslate"><span class="pre">interfaces.py</span></code></a> file. You can:</p>
<ul class="simple">
<li><p>Open a pull request to add your interface directly</p></li>
<li><p>If you’re short on time, just drop your interface in a GitHub issue and we’ll add it for you</p></li>
</ul>
</div>
<div class="section" id="walkthrough">
<h3>Walkthrough<a class="headerlink" href="#walkthrough" title="Permalink to this heading"></a></h3>
<p>Let’s go through what happens in the example above step by step:</p>
<p><strong>1. Define adapter methods to plug into a model:</strong><br />
The <code class="docutils literal notranslate"><span class="pre">adapter_methods</span></code> argument is the central parameter to configure which adapters will be supported in the model.
Here, we enable all LoRA and ReFT based adapters.
See <a class="reference internal" href="classes/adapter_model_interface.html#adapters.AdapterMethod" title="adapters.AdapterMethod"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterMethod</span></code></span></a> for valid options to specify here.
Check out <a class="reference internal" href="methods.html"><span class="std std-doc">Adapter Methods</span></a> for detailed explanation of the methods.</p>
<p><strong>2. Define layer and module names:</strong><br />
While all Transformers layers share similar basic components, their implementation can differ in terms of subtleties such as module names.
Therefore, the <a class="reference internal" href="classes/adapter_model_interface.html#adapters.AdapterModelInterface" title="adapters.AdapterModelInterface"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterModelInterface</span></code></span></a> needs to translate the model-specific module structure into a common set of access points for adapter implementations to hook in.
The remaining attributes in the definition above serve this purpose.
Their attribute names follow a common syntax that specify their location and purpose:</p>
<ul class="simple">
<li><p>The initial part before the first “_” defines the base module relative to which the name should be specified.</p></li>
<li><p>The remaining part after the first “_” defines the functional component.</p></li>
</ul>
<p>E.g., <code class="docutils literal notranslate"><span class="pre">model_embeddings</span></code> identifies the embeddings layer (functional component) relative to the base model (location).
<code class="docutils literal notranslate"><span class="pre">layer_output_proj</span></code> identifies the FFN output projection relative to one Transformer layer.
Each attribute value may specify a direct submodule of the reference module (<code class="docutils literal notranslate"><span class="pre">&quot;embed_token&quot;</span></code>) or a multi-level path starting at the reference module (<code class="docutils literal notranslate"><span class="pre">&quot;mlp.down_proj&quot;</span></code>).</p>
<p><strong>3. (optional) Extended interface attributes:</strong><br />
There are a couple of attributes in the <a class="reference internal" href="classes/adapter_model_interface.html#adapters.AdapterModelInterface" title="adapters.AdapterModelInterface"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterModelInterface</span></code></span></a> that are only required for some adapter methods.
We don’t need those in the above example for LoRA and ReFT, but when supporting bottleneck adapters as well, the full interface would look as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">adapter_interface</span> <span class="o">=</span> <span class="n">AdapterModelInterface</span><span class="p">(</span>
    <span class="n">adapter_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bottleneck&quot;</span><span class="p">,</span> <span class="s2">&quot;lora&quot;</span><span class="p">,</span> <span class="s2">&quot;reft&quot;</span><span class="p">],</span>
    <span class="n">model_embeddings</span><span class="o">=</span><span class="s2">&quot;embed_tokens&quot;</span><span class="p">,</span>
    <span class="n">model_layers</span><span class="o">=</span><span class="s2">&quot;layers&quot;</span><span class="p">,</span>
    <span class="n">layer_self_attn</span><span class="o">=</span><span class="s2">&quot;self_attn&quot;</span><span class="p">,</span>
    <span class="n">layer_cross_attn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">attn_k_proj</span><span class="o">=</span><span class="s2">&quot;k_proj&quot;</span><span class="p">,</span>
    <span class="n">attn_q_proj</span><span class="o">=</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span>
    <span class="n">attn_v_proj</span><span class="o">=</span><span class="s2">&quot;v_proj&quot;</span><span class="p">,</span>
    <span class="n">attn_o_proj</span><span class="o">=</span><span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
    <span class="n">layer_intermediate_proj</span><span class="o">=</span><span class="s2">&quot;mlp.up_proj&quot;</span><span class="p">,</span>
    <span class="n">layer_output_proj</span><span class="o">=</span><span class="s2">&quot;mlp.down_proj&quot;</span><span class="p">,</span>
    <span class="n">layer_pre_self_attn</span><span class="o">=</span><span class="s2">&quot;input_layernorm&quot;</span><span class="p">,</span>
    <span class="n">layer_pre_cross_attn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">layer_pre_ffn</span><span class="o">=</span><span class="s2">&quot;pre_feedforward_layernorm&quot;</span><span class="p">,</span>
    <span class="n">layer_ln_1</span><span class="o">=</span><span class="s2">&quot;post_attention_layernorm&quot;</span><span class="p">,</span>
    <span class="n">layer_ln_2</span><span class="o">=</span><span class="s2">&quot;post_feedforward_layernorm&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>4. Initialize adapter methods in the model:</strong>
Finally, we just need to apply the defined adapter integration in the target model.
This can be achieved using the usual <code class="docutils literal notranslate"><span class="pre">adapters.init()</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">adapters</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="n">adapter_interface</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, you can use (almost) all functionality of the <em>Adapters</em> library on the adapted model instance!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some models like GPT-2 or ModernBERT have the query, value and key layer in one single tensor. In this case, you must set the <cite>attn_qkv_proj</cite> instead of setting <cite>attn_k_proj</cite>, <cite>attn_q_proj</cite> and <cite>attn_v_proj</cite>.</p>
</div>
</div>
</div>
<div class="section" id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this heading"></a></h2>
<p>The following features of the <em>Adapters</em> library are not supported via the plugin interface approach:</p>
<ul class="simple">
<li><p>Prefix Tuning adapters</p></li>
<li><p>Parallel composition blocks</p></li>
<li><p>XAdapterModel classes</p></li>
<li><p>Setting <code class="docutils literal notranslate"><span class="pre">original_ln_after=False</span></code> in bottleneck adapter configurations (this affects <code class="docutils literal notranslate"><span class="pre">AdapterPlusConfig</span></code>)</p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="model_overview.html" class="btn btn-neutral float-left" title="Model Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="classes/models/albert.html" class="btn btn-neutral float-right" title="ALBERT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, AdapterHub Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <!--- IMPORTANT: This file has modifications compared to the snippet on the documentation page! -->
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Versions</span>
    v: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><a href="plugin_interface.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>